% This file is iccc.tex.  It contains the formatting instructions for and acts as a template for submissions to ICCC.  It borrows liberally from the AAAI and IJCAI formats and instructions.  It uses the files iccc.sty, iccc.bst and iccc.bib, the first two of which also borrow liberally from the same sources.


\documentclass[letterpaper]{article}
\usepackage{iccc}


\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

% Packages maison
\usepackage{amsmath,amssymb} % For including math equations, theorems, symbols, etc
\usepackage{bm}
\usepackage{graphicx} % Required for including images
\graphicspath{{Figures/}} % Set the default folder for images
\usepackage{prettyref}

\pdfinfo{
/Title (Formatting Instructions for Authors)
/Subject (Proceedings of ICCC)
/Author (ICCC)}
% The file iccc.sty is the style file for ICCC proceedings.
%
\title{The Live Orchestral Piano : a conditional model for musical orchestration}
\author{Léopold Crestel, Philippe Esling\\
Equipe Représentation Musicales\\
Institut de Recherche et Coordination Acoustique/Musique\\
1 Place Igor-Stravinsky, 75004 Paris\\
leopold.crestel@ircam.fr, philippe.esling@ircam.fr\\
}
\setcounter{secnumdepth}{0}

\begin{document} 
\maketitle
\begin{abstract}
\begin{quote}
This paper introduces several statistical models for the automatic projective orchestration of a piano score. We evaluate the RBM, CRBM and FGCRBM models on a short-term frame-level predictive task well-known in time-series modelling. We show the limits of this evaluation for musical sequences, propose an alternative event-level based task, and evaluate the same models in this new framework. We eventually introduce a \textit{Max/MSP} based system that perform in real-time the orchestration of notes played on a midi keyboard.
\end{quote}
\end{abstract}

\section{Introduction}
% Orchestration classique
%% Musical orchestration : why is it so hard ?
\textit{Musical orchestration} is the subtle art of writing musical pieces for the orchestra, by combining the different instruments in order to create a particular sound. A classic exercise, which has actually been used by many famous composer as writing method, consists in two steps. An harmonic, melodic, and rhythmic structure is first written in the form of a piano score (or equivalently any polyphonic instrument). This structure is then projected on an orchestra, by spreading the harmonic, melodic and rhythmic structure over the different instruments. This second step, the mapping from a piano score to an orchestral score, is often referred to as \textit{projective orchestration}. Given the number of different instruments in a symphonic orchestra and their respective range of notes, one can foresee the large number of possible combination implied.

Beside this combinatorial problem, the resulting sound of an even simple instrumental mixture is particularly difficult to predict, especially its emerging timbre. Several attempts to mathematically characterize the timbre as a set of spectro-temporal descriptors have outputted the complex non linear behaviours involved. Furthermore, an interesting orchestration will generally not simply be the allocation of the notes written on the piano score over the different instrument but will imply extension of the harmony and doubling some note with the octave. Eventually, orchestration is often referred to as the art of manipulating instrumental timbres, which lacks of both proper musical notation and a precise mathematical definition.
Those difficulties have probably been a major obstacle toward the construction of a theory of orchestration. Then it remains a mainly empirical discipline, taught through the observation of already existing orchestrations (\cite{piston-orch}).

Our objective is to build a system able to automatically perform projective orchestration. Its input is then a piano score and its output an orchestra score. To our best knowledge, this problem has never been tackled before.

% Inference stat for orchestration
If our inability to define precise rules discard the construction of a \textit{handmade} rule based system, it does not mean that underlying regularities could not be extracted. The classic and romantic repertoires contain a large number of examples of projective orchestration (e.g. the piano reduction by Liszt of the symphonies of Beethoven, \textit{les tableaux d'une exposition}, a piano piece from Modest Moussorgsky orchestrated by Ravel). Using statistical inference on a corpus constituted by piano scores and their projective orchestration by famous composers seems to be a promising lead. We believe that an appropriate statistical model would be able to extract the complex correlations that that occur along both time pitch and instrument dimensions and that human are unable to fully understand.

% Only symbolic justification (ici ou dans le texte ???)
%It has been pointed out that orchestration is art timbre blablablabl Besides, we hypothesize that in the purely symbolic information contained in the score written by composer lie knowledge about acoustic properties. Considering that all the information about instrumental mixtures is contained at a symbolic level is a strong assumption, but not a non-sense since those examples have been composed based on a certain knowledge.

\begin{figure}
\centering
\includegraphics[scale=0.16]{orch}
\caption{\textit{Projective orchestration}. A piano score is extended (projected) on an orchestra. For one piano score, many acceptable orchestration exist. Our hypothesis is that a piano score is strongly correlated to any of the orchestration that could be produced from this piece.}
\label{fig:orch}
\end{figure}

%Stat inf ?
Statistical inference refers to methods which suppose that a set of data has been drawn from a probability distribution. The objective is to deduce properties of this underlying distribution, by observing a subset of those data. In our case, the objective is to infer rules of orchestration by observing pieces written by famous composers (experts). Inferring properties about this underlying probability distribution is called the \textit{learning} phase.
In our context, it can be seen as trying to find the correlations between a piano score and an orchestral score observed in the training set. Note that for one original piano score, many different orchestrations are acceptable with no objective criterion to rank them. The probabilistic framework perfectly fits this diversity of solution.

% Deep learning :
% Puissant = modèle distrbutions complexes
% (Fast) generation
% Knowledge extraction
Statistical inference covers a wide range of domains. Among them, deep learning recently appeared as a most promising field in intelligence artificial and representation learning. 
%Its impressive results when dealing with huge amount of data, and high dimensional problems let us think that it might be a particularly fitted method for projective orchestration of symbolic sequences. In particular, promising results in text modelling (ref text modeling sutskever...) and symbolic music generation (ref boulanger) are encouraging, since those work rely on data representations very close to the one we used. Indeed, we worked with a \textit{pianoroll} representation which leads to sequences of sparse high-dimensional binaries vectors.

% Which model and why ?
We propose three models specified by a data representation and a probabilistic model. Scores are represented by a piano-roll \prettyref{fig:pianoroll}. For orchestra scores, the piano-rolls of each instruments are concatenated along the pitch dimension in order to form a single piano-roll. We To model the multi-dimensional time series obtained from that representation we experimented a \textit{RBM}, a conditional RBM (\textit{cRBM}) and Factored Gated \textit{cRBM} (\textit{FGcRBM}) \cite{taylor2009composable}.
Those models implement a notion of context, which is useful for representing both time dependencies and influence of the piano score over the orchestra score, and are generative, which means that a model directly represents the distribution of the data (by opposition to discriminative, used sometimes in classification or prediction tasks). In practice, it means that once trained, samples can be drawn from the model and then generate data similar to the one contained in the training database.

% DANS LE TEXTE :
%While being able to model complex distributions through latent units, those models implement a notion of context which allow us to model both the influence of the past over the present (temporal dimension) and of the piano over the orchestra.
% Generatif
%Those model also meet our expectation by being able to generate automatically orchestration after the learning phase. Mathematically, it means that we are able to sample from the distribution learned by the model. If correctly trained, the sampled data will boast structuring elements present in the training set of data. Those models are then called \textit{generatives}. Evaluate generatives models is complicated because of the lack of objective criterion. As pointed out before, for a given piano score, many different orchestration can be proposed and none of them are better than the other. 

%% Datasets, how to deal with them
%The test dataset must present the same characteristics of the training dataset while being different. Usually, the set of data we want to model is partitioned into three subsets called training, validating and testing datasets. The validating set is used to define a stopping criterion during the training phase, while the test dataset is used for evaluation purposes. The likelihood of a training  test under the modelled distribution is probably the best evaluation measure for generative models. It is the probability that those "ideal" set of data have been generated by the trained system. Unfortunately, the likelihood cannot be easily computed for the proposed models.
%% Evaluation framework
%To evaluate their performance, we define for the first time a quantitative evaluation framework for projective orchestration systems. We took inspiration from the many previous works in automatic music composition (REFS BOULanger ET BAHCLSTM). Most of them rely on a frame-level predictive task based on an accuracy measure.
%This evaluation framework originally defined for music prediction is then extended to the projective orchestration task.
%We highlighted a major flaw in this evaluation framework and proposed a modified version based on an event-level instead of a frame-level measurement.
%Thanks to this objective criterion, we could explore (through a grid-search, this is the ugly truth...) the hyper-parameter space, select a configuration and compare the different models between them : \textit{cRBM}, \textit{FGcRBM}, and baseline models (random generation and repeat of the previous frame).

% LOP
An interesting property of the \textit{cRBM} and \textit{FGcRBM} models is that their generative step is sufficiently fast for real-time application. Hence, we propose a system which orchestrate in real-time the performance of a pianist. Our implementation consists in a \textit{Max-MSP} patch that get the midi information from the piano, send it to the trained network which generates the orchestration through a Matlab script and send it back to the Max patch. The communication between the server (Matlab) and client (MAX) is done through the \textit{OSC} protocol. We called this system the Live Orchestral Piano (\textit{LOP}).

% Knowledge discovery
Rather than an objective, building an automatic system for orchestration is a starting point in the knowledge quarry task we want to accomplish. By trying to mimic famous composer, we might discover the higher level knowledge we are trying to discover. The long-term goal is to be able to extract from the system we have built a better understanding of orchestration toward its theorization.
One advantage of the \textit{cRBM} model is that we can easily observe the correlations and structures learnt by the system and then have a insight into the knowledge extracted by the system.

This paper is organized as follows. In sections 2 we introduce the state of the art in conditional models through three well known models: the RBM, the CRBM and the \textit{FGCRBM}. The orchestration projection task is presented in the section 3 along with an evaluation framework based on a frame-level accuracy measure. The previously introduced models are then evaluated in this framework and the results displayed. The section 4 introduces a real-time \textit{projective} orchestration system using the presented architectures.

\section{State of the art}
\label{sec:state_of_the_art}
In this section, the three models evaluated in this contribution are detailed. The \textit{RBM}, the \textit{cRBM} and the \textit{FGcRBM} are presented by increasing level of complexity, each model adding a new \textit{degree of freedom} to the previous one.

\subsection{Restricted-Boltzmann Machine}
\begin{figure}
\centering
\includegraphics[scale=0.7]{RBM}
\caption{The \textit{Restricted Boltzmann Machine} (RBM) is defined by its units ($i$ or $j$) and weights ($W_{ij}$). Units are divided between visible units (4 units at the bottom) and hidden units (3 top-most). Weights and units define an energy function. Training an RBM consists in lowering the energy function around the example from a training set. Inference in this model is easy to perform since the hidden (resp. visible) units are independent from each others.}
\label{fig:RBM}
\end{figure}
\textit{RBM} \cite{Hinton:2006:FLA:1161603.1161605} is a graphical probabilistic model \prettyref{fig:RBM}. It is composed by a set of units, each representing a random variable. Links between those units represents conditional dependencies between those variables and are named \textit{weights}. The units are divided between visible, denoted by the vector $\bm{v} = (v_{1},...,v_{m})$, and hidden units, $\bm{h} = (h_{1},...,h_{n})$. Visible nits generally represent the observed data, and then have the same dimension as the vectors in the training set. Hidden units model explanatory unobserved factors.
Weights between units are denoted $W_{ij}$. The joint probability of the visible and hidden units is given by $p_{model}(\bm{v},\bm{h}) = \frac{\exp^{-E(\bm{v},\bm{h})}}{Z}$ where
\begin{equation}
E(\bm{v},\bm{h}) = - \sum_{i=1}^{m} a_{i} v_{i}  - \sum_{i=1}^{m} \sum_{j=1}^{n} v_{i} W_{ij} h_{j} - \sum_{j = 1}^{n} b_{j} h_{j}
\end{equation}
is the energy function associated to the model. $Z = \sum_{v,h}\exp^{-E(v,h)}$ is a normalizing factor to probability and is called the partition function. $\bm{\theta} = \left\lbrace \bm{W} , \bm{a} , \bm{b} \right\rbrace$ is the set of parameters of the network.

In this context, training a model on a set of data means that we want the distribution of the model ($p_{model}$) to be as close as possible to the hypothetical real distribution of those data ($p_{data}$). A commonly used criterion for training this model is to maximize the likelihood of the training set. The vectors from the training set $\mathcal{D}$ are designated by $\bm{v^{(l)}}$.
Instead of maximiazing the likelihood, minimizing the negative log-likelihood is often preferred as it simplify expressions :
\begin{equation}
\mathcal{L(\bm{\theta}|\mathcal{D})}  = \frac{1}{N_{\mathcal{D}}} \sum_{\bm{v^{(l)}} \in \mathcal{D}} - \ln \left( p(\bm{v^{(l)}}|\bm{\theta})\right)
\end{equation}
where $N_{\mathcal{D}}$ is the size of the dataset. 

The search for the minimum of a non-linear function can be tackled by using gradient descent (REFERENCE). The gradient of the negative log-likelihood of a vector from the training database $\bm{v}^{(l)}$ is given by
\begin{equation}
\begin{split}
- \frac{\partial \ln \left[ p(\bm{v^{(l)}}|\bm{\theta})\right]}{\partial \bm{\theta}} 
= 
\mathbb{E}_{p(\bm{h}|\bm{v^{(l)}})} \left[ \frac{\partial E(\bm{v^{(l)}},\bm{h})}{\partial \bm{\theta}} \right] 
- \\
\mathbb{E}_{p(\bm{v} , \bm{h})} \left[ \frac{\partial E(\bm{v},\bm{h})}{\partial \bm{\theta}} \right]
\end{split}
\end{equation}
It is noteworthy to mention that it is formed by the difference between two expectation of the same quantity. The expectation on the left is often referred to as the \textit{data driven} term since it is an expectation over the distribution of the hidden units conditionally on a visible sample from the data distribution. 
The expectation on the right (minus sign) is referred to as the \textit{model driven} term since it is an expectation over the joint distribution of the model.
Unfortunately this quantity intractable because of the model driven term which involves a sum over all the possible configurations of the hidden (alternatively visible) units in order to compute the partition function (REF).

A training algorithm called \textit{CD} \cite{hinton2002training} rely on an approximation of the model driven term by running a k-step Gibbs chain to obtain a sample $\bm{v}^{(l,k)}$
\begin{equation}
\label{eq:grad_log_like}
- \frac{\partial \ln \left[ p(\bm{v^{(l)}}|\bm{\theta})\right]}{\partial \bm{\theta}}
\approx 
\mathbb{E}_{p(\bm{h}|\bm{v^{(l)}})} \left[ \frac{\partial E(\bm{v^{(l)}},\bm{h})}{\partial \bm{\theta}} \right] 
- 
\mathbb{E}_{p(\bm{h} | \bm{v^{(l,k)}})} \left[ \frac{\partial E(\bm{v^{(l,k)}},\bm{h})}{\partial \bm{\theta}} \right]
\end{equation}

Running a Gibbs sampling chain consists in alternatively sampling the hidden units knowing the visible units and the visible knowing the hidden \prettyref{eq:marginal_RBM}.
\begin{align}
\label{eq:marginal_RBM}
p(v_{i}=1|\bm{h}) &= \sigma \left( a_{i} + \sum_{j}W_{ij}h_{j} \right)\\
p(h_{j}=1|\bm{v}) &= \sigma \left( b_{j} + \sum_{i}W_{ij}v_{i} \right)
\end{align}
where $\sigma	(x) = \frac{1}{1+e^{-x}}$ is the \textit{sigmoid} function. Note that sampling from the marginal distribution is easy since visible units (respectively hidden units) are independent from each others. Hence, knowing the hidden units, all the visible units can be sampled in one step. This allows for a fast implementation of the samplings through matrix calculus, known as \textit{block sampling}.
It has been proved \cite{bengio2009learning} that the samples we obtain after an infinite number of iteration will be drawn from the joint distribution of the visible and hidden units of our model. Another approximation consists in starting the Gibbs chain from the sample $\bm{v}^{(l)}$, which increases the convergence of the chain, and limiting the number of sampling steps to a fixed number K. After evaluating the statistics for the distribution ($\bm{h} \sim p(\bm{h}|\bm{v^{(l)}})$ and $\bm{v^{(l,k)}}$ and $\bm{h}\sim p(\bm{h}|\bm{v^{(l)}})$ from the Gibbs sampling chain), the parameters can be updated. The whole algorithm is called Contrastive Divergence-K (CD-K). In a \textit{RBM} the update rules are given by
\begin{align}
\Delta W_{ij} &= <v_{i}h_{j} >_{data} - <v_{i}h_{j} >_{model}\\
\Delta a_{i} &= <v_{i}>_{data} - <v_{i}>_{model}\\
\Delta b_{j} &= <h_{j} >_{data} - <h_{j} >_{model}
\end{align}

\subsection{Conditional RBM}
\begin{figure}
\centering
\includegraphics[scale=0.3]{CRBM_orchestration}
\caption{\textit{Conditional RBM} adds a layer of context units to the standard RBM architectures. Those context units linearly modify the bias of both visible and hidden units.}
\end{figure}
The Conditional \textit{RBM} model \cite{taylor2009composable} is an extension of the \textit{RBM} in which dynamic biases are added to the static biases of the visible and hidden units (respectively $\bm{a}$ and $\bm{b}$). This dynamic bias linearly depends on a set of units called context units $(\bm{x})$. Those context units can be used to represent the influence on the visible units of any other factor. Typically, when trying to model time series, those context units can be used to model the influence of the recent past frames over the current frame. Hence, if we now consider that the visible units are given for a certain time frame $\bm{v}(t)$, context units can be defined as the concatenation of the N last time frames $\bm{x}(t) = \left( v_{1}^{(t)} , ... , v_{m}^{(t)}, ... , v_{1}^{(t-N)} ... , v_{m}^{(t-N)} \right)$, where N denotes the temporal order of the model.
The energy function of the Conditional RBM is given by
\begin{equation}
\label{eq:energy_CRBM}
E(\bm{v}(t),\bm{h}(t)|\bm{x}(t)) = - \sum_{i} \hat{a}_{i}(t)v_{i}(t) - \sum_{ij}W_{ij}v_{i}(t)h_{j}(t) - \sum_{j} \hat{b}_{j}(t)h_{j}(t)
\end{equation}
where the biases are defined by 
\begin{align*}
\hat{a}_{i}(t) &= a_{i} + \sum_{k}A_{ki}x_{k}(t)\\
\hat{b}_{j}(t) &= b_{j} + \sum_{k}B_{kj}x_{k}(t)
\end{align*}
We will refer to the matrices $\bm{A}$ and $\bm{B}$ as \textit{auto-regressive matrices}.

This model can be trained using CD, since the marginal probabilities of visible and hidden units are the same as the \textit{RBM}, simply replacing the static biases by dynamics ones.
The following update rules are unchanged for $\bm{W}$, $\bm{a}$ and $\bm{b}$, and are the following for the auto-regressive matrices
\begin{align}
\Delta A_{ik} 	&=<v_{i}x_{k} >_{data} - <v_{i}x_{k} >_{model}\\
\Delta B_{jk} 	&= <h_{j}x_{k} >_{data} - <h_{j}x_{k} >_{model}\\
\end{align}

%\subsection{Factored Gated cRBM}
%\begin{figure}
%\centering
%\includegraphics[scale=0.25]{FGCRBM_orchestration}
%\caption{\textit{FGCRBM} model. The features units ($\bm{z}$) modify the energy landscape of the model through a multiplicative influence over the weights $\bm{A}$, $\bm{B}$ and $\bm{W}$. Here, the role of each unit in the context of orchestration is indicated.}
%\label{fig:FGCRBM}
%\end{figure}
%The Factored Gated cRBM model (FGcRBM) \cite{taylor2009factored} proposes to extend the cRBM model by adding a layer of feature units $\bm{z}$ which modulate the weights of the conditional architecture in a multiplicative way. Hence, the parameters of the networks become $\bm{\theta} = \left\lbrace \bm{W} , \bm{A} , \bm{B} , \bm{a} , \bm{b} \right\rbrace$, where $\bm{W} = (W)_{ijl}$, $\bm{A}=(A)_{ikl}$ and $\bm{B}=(B)_{jkl}$ are three-dimensional tensors.
%
%This multiplicative influence can be interpreted as a modification of the energy function ($E$) of the model. Each configuration of the feature units defines a new energy function of the simple cRBM model defined by the other units ($\bm{v}$, $\bm{h}$, and $\bm{x}$). Since the number of parameters to train becomes high, the three dimensional tensors can be factorized into a product of three matrices by including factor units indexed by $f$ : $W_{ijl} = W_{if} . W_{jf} . W_{lf}$.
%The energy function of this Factored Gated Conditional RBM is then given by
%\begin{equation}
%E(\bm{v}(t),\bm{h}(t)|\bm{x}(t),\bm{z}(t)) = -\sum_{f}\sum_{ijl} W_{if}^{v} W_{jf}^{h} W_{lf}^{z} v_{i}(t) h_{j}(t) z_{l}(t) 
%- \sum_{i} \hat{a}_{i}(t)v_{i}(t) - \sum_{j} \hat{b}_{j}(t)h_{j}(t)
%\end{equation}
%where the dynamic biases of the visible and hidden units are defined by
%\begin{equation}
%\hat{a}_{i}(t) = a_{i} + \sum_{m} \sum_{kl}A_{im}^{v}A_{km}^{x}A_{lm}^{z}x_{k}(t)z_{l}(t)
%\end{equation}
%\begin{equation}
%\hat{b}_{j}(t) = b_{j} + \sum_{n} \sum_{kl}B_{jn}^{h}B_{kn}^{x}B_{ln}^{z}x_{k}(t)z_{l}(t)
%\end{equation}
%
%The \textit{FGCRBM} model can be trained by contrastive divergence which lead to the following update rules for the parameter
%\begin{align*}
%\Delta b_{i}^{(v)} &= <v_{i}>_{data} - <v_{i}>_{model}\\
%\Delta b_{j}^{(h)} &= <h_{j} >_{data} - <h_{j} >_{model}\\
%\Delta W_{if}^{v} &= <v_{i}\sum_{j} W_{jf} h_{j} \sum_{l} W_{lf} z_{l}>_{data} - <v_{i}W_{jf} h_{j} \sum_{l} W_{lf} z_{l} >_{model}\\
%\Delta W_{jf}^{h} &= <h_{j}\sum_{i}W_{if}v_{i} \sum_{l} W_{lf} z_{l}>_{data} - <h_{j}\sum_{i}W_{if}v_{i} \sum_{l} W_{lf} z_{l}>_{model}\\
%\Delta W_{lf}^{z} &= <z_{l}\sum_{i}W_{if}v_{i} \sum_{j} W_{jf}h_{j}>_{data} - <z_{l}\sum_{i}W_{if}v_{i} \sum_{j} W_{jf}h_{j}>_{model}\\\Delta A_{im}^{v} &= <v_{i}\sum_{k}A_{km}x_{k} \sum_{l}A_{lm}z_{l}>_{data} - <v_{i}\sum_{k}A_{km}x_{k} \sum_{l}A_{lm}z_{l}>_{model}\\
%\Delta A_{km}^{x} &= <x_{k}\sum_{i}A_{im}v_{i} \sum_{l}A_{lm}z_{l}>_{data} - <x_{k}\sum_{i}A_{im}v_{i} \sum_{l}A_{lm}z_{l}>_{model}\\
%\Delta A_{lm}^{z} &= <z_{l} \sum_{i}A_{im}v_{i} \sum_{k}A_{km}x_{k}>_{data} - <z_{l} \sum_{i}A_{im}v_{i} \sum_{k}A_{km}x_{k}>_{model}\\
%\Delta B_{jn}^{z} &=  <h_{j} \sum_{k}B_{kn}x_{k} \sum_{l}B_{ln}z_{l}>_{data} - <h_{j} \sum_{k}B_{kn}x_{k} \sum_{l}B_{ln}z_{l}>_{model}\\
%\Delta B_{kn}^{z} &= <x_{k} \sum_{j}B_{jn}h_{j} \sum_{l}B_{ln}z_{l}>_{data} - <x_{k} \sum_{j}B_{jn}h_{j} \sum_{l}B_{ln}z_{l}>_{model}\\
%\Delta B_{ln}^{z} &= <z_{l} \sum_{j}B_{jn}h_{j} \sum_{k}B_{kn}x_{k}>_{data} - <z_{l} \sum_{j}B_{jn}h_{j} \sum_{k}B_{kn}x_{k}>_{model}
%\end{align*}

\subsection{Generative models}
\begin{figure}
\centering
\includegraphics[scale=0.3]{FGCRBM_sampling}
\caption{\textit{Sampling in a FGCRBM}. Context and Features units are respectively clamped to the last ($t-1$ to $t-N$) orchestral frames and the current ($t$) piano frame. Visible units are randomly initialized. Then, several Gibbs sampling step are performed, in our case 40.}
\label{fig:FGCRBM_sampling}
\end{figure}
%% Role des unités conditionelles et tout le tralala dans les modèles
Once trained on a dataset using CD, those models represent a probability distribution $p$. If correctly trained, this distribution is supposed to be close (in the sense of the Kullback-Leibler divergence \cite{hinton2002training}) to the underlying probability distribution of the data. Then, by sampling from $p$, we are able to reproduce data \textit{similar} to the one observed in the training dataset.
This sampling process is called the generative step.

Sampling from \textit{RBM}-based models is not straightforward. 

Since the partition function is intractable, the marginal probability of the hidden and visible units cannot be sampled.
The generation process can be described as follow. After randomly setting the visible units vector (for each index $i$, $\bm{v}_{i} \sim \mathcal{U}(0,1)$), alternate Gibbs sampling is performed in order to approximate the joint distribution of the hidden and visible units conditionally on the context units. Given the context units, one step of alternate Gibbs sampling consists in sampling the hidden units knowing the visible, then sampling the visible units knowing the hidden. In theory the visible sample obtained is from the model distribution after an infinite number of steps. If theoretically an infinite number of steps is necessary, 20 to 100 steps are typically used in practice.

\subsection{Approximations}
In practice, several approximations are made both during the training and generative phases. The CD-K algorithm and its approximation, which consist in performing only a limited number of sampling steps and for the training phase starting the Gibbs chain with a sample from the training set, has already been discussed.

\subsubsection{Mean-field values} When evaluating the data and model-driven terms in equation \ref{eq:grad_log_like}, the two terms are the expectation of the same quantity under different distribution. For the data-driven term, this expectation is analytically derivable through the marginal probability of the hidden units, and ($\mathbb{E}\left[\bm{h}\right] = \sigma(\frac{1}{\bm{b} + \sum_{i} W_{ij}v_{i}})$).
For the model-driven term, expectation cannot be easily calculated since we do not have access to the joint probability. Approximating the expectation by running N Gibbs chains and computing an estimator would be too time consuming. Hence, a first idea is to "approximate" the expectation over $p(h|v^{(l,k)})$  by the value of the hidden vector obtained in the last step of the Gibbs chain ($\bm{h} \sim p(\bm{v^{(l,k)}})$).

Following the mathematical definition of an \textit{RBM}, the visible and hidden units must be sampled (thus equal to binaries values) from the marginal probabilities at each step of the Gibbs chain. However, it is possible to speed up the convergence of the algorithm by replacing the sampling value by the probability itself when updating the hidden vector at the last step k of the Gibbs chain : $\bm{h} = p(\bm{v^{(l,k)}})$. Doing this, we get rid of the sampling noise since the mean value of the marginal distribution is picked \cite{hinton2010practical}. Note that it is also possible to do so with the visible units at each time step, but it has been shown that it leads to a worse density estimation, which  is not a problem when the algorithm is used only as a pre-training step before 

\subsubsection{Number of sampling steps} It is interesting to note that $K=1$ in the CD-K algorithm during the training phase. This is because the objective is to modify the energy of our model in order to increase the likelihood  of the training set under the model distribution. It has been shown that even a small number of sampling steps guarantees to increase a lower bound over the likelihood \cite{bengio2009learning}. When generating data, we seek to obtain a sample from the distribution of the model. Therefore, a larger number of Gibbs sampling steps need to be performed in order to effectively obtain a good approximation.

\section{Projective orchestration}
In this section, we present how to perform automatic projective orchestration using a cRBM. In particular, we detail which kind of data representation we used and the modelling function of the different units.
A objective criterion of the performances of a model is necessary. Not only to compare the different systems, but also to find the best set of hyper-parameter for a given model. To our best knowledge, there is no quantitative evaluation framework for automatic projective orchestration. We propose here a first attempt in order to fill this gap by defining a projective orchestration inference task. Given a test database composed by piano scores and orchestrations proposed by experts (famous composer), it consists at each time frame $t$ to generate an orchestral vector $\hat{Orch}(t)$ knowing the piano frame $Piano(t)$ and the recent past of sequence of orchestral vectors $Orch(t-1),... Orch(t-N)$. The predicted vector $\hat{Orch}(t)$ is then compared to the ground-truth $Orch(t)$ via an accuracy measure.
The database used and results obtained with the \textit{cRBM} are then presented in the two last sections.

\subsection{Formalization}
Conditional models allow to generate sequences of data under a certain context. Projective orchestration can be seen as producing an orchestral score, conditionally on a piano score. In order to guarantee a form of temporal continuity in the orchestration, the recent past of the orchestral sequence is added in the contextual information.

\subsubsection{Data representation}
\label{sec:data_representation}
Before being able to train the previously introduced models, the first step is to choose an adapted representation for the piano and orchestra information.
\begin{figure}
\centering
\includegraphics[scale=0.55]{data_representation}
\caption{The successive visible units of a neural network could be the successive temporal frames of a \textit{pianoroll}. A \textit{pianoroll} is a representation of musical events, discrete on both the frequency (pitch) and the time (frames) scales. For a single instrument, a pitch $p$ at time $t$ can be either played or not, which is represented by a one or zero in the \textit{pianoroll}. This definition is extended to an orchestra by simply concatenating the \textit{pianorolls} of each instruments along the pitch dimension. One can see on the figure that the same instruments are grouped together event if they don't play the same thing. For instance, trumpets 1, 2, 3 and 4 are grouped under one trumpet part, which then contains 4 voices chords.}
\label{fig:pianoroll}
\end{figure}
The \textit{pianoroll} representation is often used to model sequences of symbolic music.
this is a binary matrix $Pianoroll(p,t)$ of dimension $N_{T} \times N_{P}$.
For a single instrument $N_{T}$ is the temporal length of the musical sequence,  and $N_{P}$ the number of pitches potentially played by the instrument. Hence, for a certain time quantisation $Pianoroll(p,t)$ specifies if a pitch $p$ is played at time frame $t$.
The dynamics are ignored and each time frame can be represented by a binary vector $Pianoroll(t) \in \left\lbrace 0  1 \right\rbrace ^{N_{P}}$.
As depicted in figure \ref{fig:pianoroll}, this representation can be easily extended to an orchestra composed by $N_{I}$ instruments by simply concatenating the \textit{pianorolls} of each instrument over the pitch dimension.

In the proposed system, two different \textit{pianorolls} are used to represent the piano and the orchestra score.
For the piano score, $N_{P} = 88$ since this is the number of keys for a standard grand piano.
The orchestra representation require more attention. Each instrument has a different playing range. Hence, the size of their pitch dimension is different for each. In practice, we limited it to the pitch observed in the training dataset. The observed range is denoted $R(i)$ for each instrument $i$ in an set of instrument $I = {\text{violin},\text{viola},...}$. If the training base is sufficiently large, every note that can be played by each instrument will be seen at least once. This restriction acts as an important constraint over the playing range of each instrument by preventing the system to assign absurd notes to the different instrument (for instance a low register note to a piccolo).

Note that we follow the usual orchestral simplifications used when writing orchestral scores by grouping together all the instruments of a same section. For instance, the section \textit{violin 1}, composed by many instrumentalists (10 or more), is written as a single part.
Eventually, following the notations we just introduced, the dimension of an orchestra \textit{pianoroll} composed by the instrument set $I$ are given by $N_{T} \times \sum_{i \in I} R(i)$.

In our framework we chose 14 instruments indexed by 
\begin{figure}
\begin{center}
\begin{tabular}{|r|r|c|r|r|}
\cline{1-2}
\cline{4-5}
\rule{0pt}{2.5ex} Index & Instrument & & Index & Instrument\\
\cline{1-2}
\cline{4-5}
\rule{0pt}{2.5ex} 1 & Violin & & 8 & Trombone\\
2 & Viola & & 9 & Tuba\\
3 & Cello &  & 10 & French horn\\
4 & Double-bass & & 11 & Oboe\\
5 & Harp & & 12 & Bassoon\\
6 & Timpani & & 13 & Clarinet\\
7 & Trumpet & & 14 & Flute\\
\cline{1-2}
\cline{4-5}
\end{tabular}
\end{center}
\caption{Indexation used for the orchestra instruments}
\end{figure}

\subsubsection{Modeling orchestral sequences}
For a piece of music, we define two sequences of vectors $Orch(t)$ and $Piano(t)$ with $t$ in $\left[ | 1 , N_{T} | \right]$ where $N_{T}$ is the lenght of the piece. Those are respectively defined by the sequence of column vector from the \textit{pianoroll} representation of the orchestra part and of the piano part.

At each time frame $t$, the visible units of the cRBM represent the current orchestral vector ($Orch(t)$), conditional units are used to model the influence of the past orchestral vectors $Orch(t-1) , ... , Orch(t-N)$ and the influence of the current piano frame ($Piano(t)$) over the visible units. The context units are then defined by the concatenation of the past orchestral frames and the current piano frame
$ Context(t) = \left[ Piano(t) , Orch(t-1) , ... , Orch(t-N)\right]$.

%The \textit{FGCRBM} model allows to separate the influence of the current piano frame and the past orchestral frames. The current piano frame defines the feature units ($z$) $ Features(t) = Piano(t)^{T} $, and the concatenation of the past orchestral frames define the context units ($x$) $ Context(t) = \left[ Orch(t-1)^{T} , ... , Orch(t-N)^{T} \right]^{T}$ (\prettyref{fig:FGCRBM}).

Note that we do not restrain the possible pitches of each instrument to the pitches seen in the piano score at each specific time frame. Indeed, orchestrating a piano score cannot be reduced to the simple repartition between the different instruments of the same notes already written in the piano score. Instead, the harmonic structure is often enriched by extra notes, ranging from simple octaviations to harmonic expansions (more complex chords with a specific colour). The melody might be doubled by several instruments at the unison, octave or even double octave.
+ Something about the phrasing ? A wind instru can't play the same phrasing as a pianist. Reciprocally, a pianist can't tremolo

\subsection{Evaluation}
Building a quantitative evaluation framework for generative models is rarely straightforward, especially since computing the likelihood of a test sample is intractable in the models we used. A common practice is to define an auxiliary task close to the generative objective but easier to evaluate.
Typically, in automatic music generation, the models are evaluated through a predictive task based on a frame-level accuracy measure \cite{DBLP:journals/corr/LiuR14a,boulanger2012modeling,lavrenko2003polyphonic}. We introduce an extension of this evaluation framework to an orchestral context through a task called \textit{frame-level orchestral inference}. This framework heavily relies on the previous works in automatic music generation and we discovered a bias in the measurement due to the temporal scale used (frame-level). We then propose a new evaluation framework and present the results of our system in a newly introduced \textit{event-level} framework.

\subsubsection{Frame-level accuracy}
The frame-level accuracy of a model is defined as the mean value of the accuracies measured for each time frame of a testing set.
For each time frame, we try to predict the orchestral frame $\hat{Orch}(t)$ knowing the recent past $Orch(t-1),...,Orch(t-N)$ and the piano frame $Piano(t)$ and compare it to the original frame $Orch(t)$. The accuracy measure the difference between the predicted and original frames \cite{boulanger2012modeling,DBLP:journals/corr/LiuR14a}
\begin{equation}
\text{Accuracy}  = \frac{TP(t)}{TP(t) + FP(t) + FN(t)}
\label{eq:accuracy}
\end{equation}
where $TP(t)$ is the number of notes correctly predicted (true positives). $FP(t)$ is the number of notes predicted which are not in the original sequence (false positive) and $FN(t)$ is the number on unreported notes (false negative). 

Instead of binary values, activation probabilities are used for the predicted samples in order to reduce the sampling noise. In the case this probability is intractable, one should sample many predicted frames for each time frame and compute the mean value of those samples.

\subsubsection{Event-level accuracy}
A major flaw in the frame-level accuracy measure is its dependence to the rhythmic quantization chosen. Indeed, when the quantization become too small, it becomes highly probable that a frame is simply repeated at the next time frame. Hence, the best predictive model is simply a model which predict that the next time frame is the same as the frame $t-1$. This is not a desirable behaviour, and we propose an evaluation based on an event-level to address that issue.

A musical event is defined as a change in the orchestral score, either a note being switched on or off. The predictive event-level accuracy measure relies on the same accuracy measure previously defined \prettyref{eq:accuracy}. The only difference is that it occurs at each new event instead of each new time frame. Since the task is slightly different, the training phase has been consequently changed so that a model is trained only on new event frames.

% Be careful, event level not on the context

\subsection{Database}
We used a parallel database of piano scores and their orchestration by famous composers. The database consists of 76 \textit{XML} files. Given the complexity of the distribution we wanted to model and the reduced size of the database we have accessed to, we decided to keep as a test dataset only the last half of one track from our database. Hence 75 and a half files were used to train our model. We chose to do so in order to have the best generation ability.
The contrastive-divergence algorithm has been applied on mini-batches of size $100$ during the training phase. Thus we obtained $335$ mini-batches for the frame-level measure, and $89$ for the event-level measure.
For each instrument, the pitch range is reduced to the \textit{tessitura} observed in the training dataset (\label{sec:data_representation}). We used a rhythmic quantization of 8 frames per beat.

\subsection{Results}
We evaluated the \textit{CRBM} and \textit{FGCRBM} previously presented \prettyref{sec:state_of_the_art} in both the frame-level and event-level frameworks. Those two models are compared to a random prediction of each frame and to a simple model that outputs the previous frame as the current frame prediction (i.e. a untrained 1 order linear predictor). The results are presented in the following tables \prettyref{fig:result_frame} and \prettyref{fig:result_event}.

% Frame
% Event
%
% Precision et accuracy
%
% Séparer repeat et change true/false positive
%%%%%%%%%%%%%%%%%%\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%\\
%%%%%%%%%\\
%%%%%%%%%%%%%%%%%%\\
% Coller une image
%\begin{figure}
%\centering
%\begin{tabular}{c c}
%\hline
%\multirow{2}{*}{Model} & Orchestral Frame-level (\%)\\
%\hline
%Random & 0.51\\ 
%Repeat & 93.25\\ 
%\hline \hline
%CRBM & 45.07\\ 
%FGCRBM & 2.05\\ 
%\end{tabular}
%\caption{Frame-level accuracy}
%\label{fig:result_frame}
%\end{figure}
%
%\begin{figure}
%\centering
%\begin{tabular}{c c}
%\hline
%\multirow{2}{*}{Model} & Orchestral Event-level (\%)\\
%\hline
%Random & 0.50\\ 
%Repeat & 93.25\\ 
%\hline \hline
%CRBM & 27.78\\ 
%FGCRBM & \\ 
%\end{tabular}
%\caption{Event-level accuracy}
%\label{fig:result_frame}
%\end{figure}
%% C'est de la merde, même le edvent-level ça règle pas du tout le problème : soit on prédit l'event mais avec un context basé sur les frames, et dans ce cas là le modèle repeat est bon, soit on prends que les event mais ça n'a aucun sens (pas de rythme...)
%%%%%%%%%
%%%%%%%%%
%%%%%%%%%\\
%%%%%%%%%
%%%%%%%%%\\


\section{Live Orchestral Piano}


\section{Conclusion and future works}
% Better DB
% Change the data representation

% Acknowledgmentsinclude

% Bibliography
\bibliographystyle{iccc}
\bibliography{biblio}

\end{document}
